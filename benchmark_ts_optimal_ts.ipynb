{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "benchmark_ts_optimal_ts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/calibration_is_all_you_need/blob/main/benchmark_ts_optimal_ts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM0-fzshDuxQ",
        "outputId": "7818ba06-476c-41cc-a869-3629d90b12dc"
      },
      "source": [
        "import numpy as np \n",
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score\n",
        "#NLP\n",
        "from torchtext import data\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "#torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import math\n",
        "import torchtext\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext import data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PByFEiMJvMNw",
        "outputId": "f81e9b99-9d16-4464-aa7a-eb2e26ab1d65"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sJ09pMzGHOQ",
        "outputId": "1cb830f7-d930-4272-fcf9-bfd8a081b5b0"
      },
      "source": [
        "train_dir = \"/content/drive/MyDrive/calibration_project/sentiment_analysis/Tweets.csv\"\n",
        "df = pd.read_csv(train_dir)\n",
        "batch_size = 8\n",
        "max_length = 35\n",
        "def remove_unnecessary(text):\n",
        "    #remove_URL\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('', text)\n",
        "\n",
        "    #remove_html\n",
        "    html = re.compile(r'<.*?>')\n",
        "    text = html.sub('', text)\n",
        "\n",
        "    #remove @\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "\n",
        "    #remove_emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    #Removes integers \n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    \n",
        "    #remove_punct\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(table)\n",
        "\n",
        "    #Replaces contractions from a string to their equivalents \n",
        "    contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), \n",
        "                            (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "                            (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), \n",
        "                            (r'dont', 'do not'), (r'wont', 'will not')]\n",
        "    \n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        text, _= re.subn(pattern, repl, text)\n",
        "\n",
        "\n",
        "\n",
        "    #lemmatize_sentence\n",
        "    sentence_words = text.split(' ')\n",
        "    new_sentence_words = list()\n",
        "    \n",
        "    for sentence_word in sentence_words:\n",
        "        sentence_word = sentence_word.replace('#', '')\n",
        "        new_sentence_word = WordNetLemmatizer().lemmatize(sentence_word.lower(), wordnet.VERB)\n",
        "        new_sentence_words.append(new_sentence_word)\n",
        "        \n",
        "    new_sentence = ' '.join(new_sentence_words)\n",
        "    new_sentence = new_sentence.strip()\n",
        "\n",
        "    return new_sentence\n",
        "\n",
        "\n",
        "def prepare_csv(df_train, seed=27, val_ratio=0.2):\n",
        "    idx = np.arange(df_train.shape[0])    \n",
        "    np.random.shuffle(idx)\n",
        "    val_size = int(len(idx) * val_ratio)\n",
        "    if not os.path.exists('cache'): # cache is tem memory file \n",
        "        os.makedirs('cache')\n",
        "    \n",
        "    df_train.iloc[idx[val_size:], :][['tweet_id', 'airline_sentiment', 'text']].to_csv(\n",
        "        'cache/dataset_train.csv', index=False)\n",
        "    \n",
        "    df_train.iloc[idx[:val_size], :][['tweet_id', 'airline_sentiment', 'text']].to_csv(\n",
        "        'cache/dataset_val.csv', index=False)    \n",
        "    \n",
        "def get_iterator(dataset, batch_size, train=True,\n",
        "                 shuffle=True, repeat=False, device=None): \n",
        "    dataset_iter = data.Iterator(\n",
        "        dataset, batch_size=batch_size, device=device,\n",
        "        train=train, shuffle=shuffle, repeat=repeat,\n",
        "        sort=False)  \n",
        "    return dataset_iter\n",
        "\n",
        "def get_dataset(fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, batch_size=batch_size, device=None): \n",
        "    print('Preparing dataset...')\n",
        "    train = pd.read_csv(train_dir,error_bad_lines=False)\n",
        "    train['airline_sentiment'] = train['airline_sentiment'].map({\n",
        "    'positive': 0,\n",
        "    'negative': 1,\n",
        "    'neutral':2})\n",
        "    # labels = train['airline_sentiment'].tolist()\n",
        "    train['text'] = train['text'].apply(lambda x: remove_unnecessary(x))\n",
        "    \n",
        "    if vectors is not None:\n",
        "        lower=True\n",
        "    #to split dataset for train and validation\n",
        "    prepare_csv(train)\n",
        "    TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),init_token='<sos>',eos_token='<eos>',lower=True,batch_first=True, \n",
        "                      fix_length=fix_length)\n",
        "    \n",
        "    LABEL = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)\n",
        "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)   \n",
        "    train_temp, val_temp = data.TabularDataset.splits(\n",
        "        path='cache/', format='csv', skip_header=True,\n",
        "        train='dataset_train.csv', validation='dataset_val.csv',\n",
        "        fields=[('tweet_id', ID), ('airline_sentiment', LABEL), ('text', TEXT)])  \n",
        "  \n",
        "    TEXT.build_vocab(\n",
        "        train_temp, val_temp,\n",
        "        max_size=20000,\n",
        "        min_freq=10)\n",
        "        #vectors=GloVe(name='6B', dim=300) )\n",
        "    LABEL.build_vocab(train_temp)\n",
        "    ID.build_vocab(train_temp, val_temp)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    \n",
        "    train_loader = get_iterator(train_temp, batch_size=batch_size, \n",
        "                              train=True, shuffle=True,\n",
        "                              repeat=False,device=device)\n",
        "    test_loader = get_iterator(val_temp, batch_size=batch_size, \n",
        "                            train=True, shuffle=True,\n",
        "                            repeat=False, device=device)\n",
        "\n",
        "    print('Train samples:%d'%(len(train_temp)), 'Valid samples:%d'%(len(val_temp)),'Train minibatch nb:%d'%(len(train_loader)),\n",
        "          'Valid minibatch nb:%d'%(len(test_loader)), 'ntokens:%d'%(ntokens))\n",
        "    \n",
        "    return vocab_size, ntokens, word_embeddings, train_loader, test_loader\n",
        "\n",
        "vocab_size, ntokens, word_embeddings, train_loader, test_loader = get_dataset (fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, batch_size=batch_size, device=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing dataset...\n",
            "Train samples:11712 Valid samples:2928 Train minibatch nb:1464 Valid minibatch nb:366 ntokens:1603\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxPKjyhRGK0K"
      },
      "source": [
        "def create_masks(sentence):\n",
        "    \n",
        "    sentence_mask = sentence!=0\n",
        "    sentence_mask = sentence_mask.to(device)\n",
        "    sentence_mask = sentence_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    return sentence_mask \n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_length, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):    \n",
        "    def __init__(self, d_model, heads, num_layers, ntokens):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = ntokens\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        #self.embed_dec = Embeddings(self.vocab_size, d_model,max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        #self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(max_length*self.d_model, 3)   \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    # def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "    #     tgt_embeddings = self.embed_dec(target_words)\n",
        "    #     for layer in self.decoder:\n",
        "    #         tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "    #     return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        #decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        #out = F.log_softmax(self.logit(encoded), dim = 2)\n",
        "        encoded = encoded.view(batch_size,-1)\n",
        "        out = self.logit(encoded)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.view(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss\n",
        "\n",
        "def seed_everything(seed=27):\n",
        "  #random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IATn-BfHKkvh"
      },
      "source": [
        "def train(train_loader, transformer, criterion, epoch,optim):    \n",
        "    transformer.train()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    for i, pair in enumerate(train_loader): \n",
        "        input = pair.text.to(device)\n",
        "        target = pair.airline_sentiment.to(device)\n",
        "\n",
        "        input_mask = create_masks(input)\n",
        "        out = transformer(input, input_mask )\n",
        "        loss = criterion(out, target.long())\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        \n",
        "\n",
        "def valid (test_loader,transformer): \n",
        "    transformer.eval()\n",
        "    sum_loss = 0\n",
        "    sum_acc = 0\n",
        "\n",
        "    for i, pair in enumerate(test_loader): \n",
        "\n",
        "        input = pair.text.to(device)\n",
        "        target = pair.airline_sentiment.to(device)\n",
        "\n",
        "        input_mask = create_masks(input)\n",
        "        out = transformer(input, input_mask)\n",
        "        loss = criterion(out, target.long())\n",
        "        sum_loss += loss.item()\n",
        "        num_corrects = (torch.max(out, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects / batch_size\n",
        "        sum_acc += acc.item()\n",
        "    return sum_loss/len(test_loader) ,  sum_acc/len(test_loader)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wubcM3ac8Se",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f89ac80e-be4d-4765-bfaa-c364f2855cb2"
      },
      "source": [
        "seed_everything()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "lr = 1e-4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "best_acc = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(10):\n",
        "    train(train_loader, model, criterion, epoch,optimizer)\n",
        "    loss, acc = valid (test_loader,model)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model, 'transformer_optimizer': optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/calibration_project/sentiment_analysis/best_base_model_object.pth.tar')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/calibration_project/sentiment_analysis/best_base_model_object.pth')\n",
        "    print('cur epoch:%d, cur acc:%.5f, best epoch:%d, best acc:%.5f'%(epoch,acc, best_epoch, best_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur acc:69.91120, best epoch:0, best acc:69.91120\n",
            "cur epoch:1, cur acc:73.22404, best epoch:1, best acc:73.22404\n",
            "cur epoch:2, cur acc:73.12158, best epoch:1, best acc:73.22404\n",
            "cur epoch:3, cur acc:74.82923, best epoch:3, best acc:74.82923\n",
            "cur epoch:4, cur acc:73.77049, best epoch:3, best acc:74.82923\n",
            "cur epoch:5, cur acc:74.55601, best epoch:3, best acc:74.82923\n",
            "cur epoch:6, cur acc:74.31694, best epoch:3, best acc:74.82923\n",
            "cur epoch:7, cur acc:72.67760, best epoch:3, best acc:74.82923\n",
            "cur epoch:8, cur acc:69.84290, best epoch:3, best acc:74.82923\n",
            "cur epoch:9, cur acc:73.63388, best epoch:3, best acc:74.82923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVFaQCwzVzhS"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, inputs, input_mask): #, targets, targets_mask):\n",
        "        logits = self.model(inputs, input_mask) #, targets, targets_mask)\n",
        "        return self.temperature_scale(logits)\n",
        "\n",
        "    def temperature_scale(self, logits):\n",
        "        # Expand temperature to match the size of logits\n",
        "        temperature = self.temperature.unsqueeze(1).expand(logits.size())\n",
        "        return logits / temperature\n",
        "\n",
        "    # This function probably should live outside of this class, but whatever\n",
        "    def set_temperature(self, valid_loader):\n",
        "        self.cuda()\n",
        "        ece_criterion = _ECELoss().cuda()\n",
        "        nll_criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "        # First: collect all the logits and labels for the validation set\n",
        "        logits_list = []\n",
        "        labels_list = []\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            for i, pair in enumerate(valid_loader):\n",
        "    \n",
        "                input = pair.text.cuda()\n",
        "                label = pair.airline_sentiment.cuda()\n",
        "                #for input, label in valid_loader:\n",
        "                input = input.cuda()\n",
        "                label = label.cuda()\n",
        "                #label = label[:, 1:]\n",
        "                input_mask = create_masks(input)\n",
        "                #input_mask, label_mask = create_masks(input, label)\n",
        "                logits = self.model(input, input_mask)#, label, label_mask)\n",
        "                logits_list.append(logits)\n",
        "                labels_list.append(label)\n",
        "            logits = torch.cat(logits_list).cuda()\n",
        "            labels = torch.cat(labels_list).cuda()\n",
        "            \n",
        "\n",
        "        # Next: optimize the temperature w.r.t. NLL\n",
        "        init_temp = self.temperature.clone()\n",
        "        optimizer = optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "        def eval():\n",
        "            labels_loss = labels.reshape(-1)\n",
        "            loss = nll_criterion(self.temperature_scale(logits.view(-1, 3)), labels_loss.long())\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(eval)\n",
        "\n",
        "        # CalculateECE after temperature scaling\n",
        "        labels_loss = labels.reshape(-1)\n",
        "        after_temperature_ece = ece_criterion(self.temperature_scale(logits.view(-1,3 )), labels_loss.long()).item()\n",
        "        print('Initial temperature: %.3f, Optimal temperature: %.3f' % (init_temp, self.temperature.item()))\n",
        "        return self\n",
        "\n",
        "\n",
        "class _ECELoss(nn.Module):\n",
        "    def __init__(self, n_bins=15):\n",
        "        \"\"\"\n",
        "        n_bins (int): number of confidence interval bins\n",
        "        \"\"\"\n",
        "        super(_ECELoss, self).__init__()\n",
        "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "        self.bin_lowers = bin_boundaries[:-1]\n",
        "        self.bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        softmaxes = F.softmax(logits, dim=1)\n",
        "        confidences, predictions = torch.max(softmaxes, 1)\n",
        "        accuracies = predictions.eq(labels)\n",
        "        ece = torch.zeros(1, device=logits.device)\n",
        "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
        "            # Calculated |confidence - accuracy| in each bin\n",
        "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "            prop_in_bin = in_bin.float().mean()\n",
        "            if prop_in_bin.item() > 0:\n",
        "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "        return ece\n",
        "\n",
        "def evaluation(model, test_loader):\n",
        "    sum_loss = 0\n",
        "    sum_acc = 0\n",
        "    logits_list = []\n",
        "    labels_list = []\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, pair in enumerate(test_loader): \n",
        "\n",
        "            input = pair.text.to(device)\n",
        "            target = pair.airline_sentiment.to(device)\n",
        "\n",
        "            input_mask = create_masks(input)\n",
        "            out = model(input, input_mask)\n",
        "            loss = criterion(out, target.long())\n",
        "            sum_loss += loss.item()\n",
        "            num_corrects = (torch.max(out, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "            acc = 100.0 * num_corrects / batch_size\n",
        "            sum_acc += acc.item()\n",
        "            logits_list.append(out)\n",
        "            labels_list.append(target)\n",
        "\n",
        "            \n",
        "        logits_all = torch.cat(logits_list).cuda()\n",
        "        labels_all = torch.cat(labels_list).cuda()\n",
        "        \n",
        "    return sum_loss/len(test_loader), sum_acc/len(test_loader), logits_all, labels_all\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqKrOCRM5C9i",
        "outputId": "7d570753-8f42-42c6-feda-69735f37cbb4"
      },
      "source": [
        "seed_everything()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "lr = 1e-4\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "#model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/sentiment_analysis/best_base_model_object.pth'),strict=False)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (embed): Embeddings(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (embed): Embedding(1603, 512)\n",
              "  )\n",
              "  (encoder): ModuleList(\n",
              "    (0): EncoderLayer(\n",
              "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_multihead): MultiHeadAttention(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (concat): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): EncoderLayer(\n",
              "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_multihead): MultiHeadAttention(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (concat): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): EncoderLayer(\n",
              "      (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (self_multihead): MultiHeadAttention(\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (concat): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (logit): Linear(in_features=17920, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JXDavuUlcoM",
        "outputId": "55861a69-743a-4917-da1d-ec65a29312a9"
      },
      "source": [
        "seed_everything()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# d_model = 512\n",
        "# heads = 8\n",
        "# num_layers = 3\n",
        "# lr = 1e-4\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, ntokens = ntokens)\n",
        "# model = model.to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# model.load_state_dict(torch.load('/content/drive/MyDrive/calibration_project/sentiment_analysis/best_base_model_object.pth'),strict=False)\n",
        "# model.eval()\n",
        "\n",
        "ece_criterion = _ECELoss().to(device)\n",
        "# loss, acc, logits_all, labels_all = evaluation(model, test_loader)\n",
        "# logits_all = logits_all.view(-1,3)\n",
        "# labels_all = labels_all.view(-1)\n",
        "# temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "# print('Before TS- acc:%.3f, bef ece:%.5f'%(acc,temperature_ece))\n",
        "\n",
        "model_ts = ModelWithTemperature(model)\n",
        "model_ts.set_temperature(test_loader)\n",
        "loss, acc, logits_all, labels_all = evaluation(model_ts, test_loader)\n",
        "logits_all = logits_all.view(-1,3)\n",
        "labels_all = labels_all.view(-1)\n",
        "temperature_ece = ece_criterion(logits_all, labels_all).item()\n",
        "print('After TS- acc:%.3f,aft ece:%.5f'%(acc,temperature_ece))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial temperature: 1.500, Optimal temperature: 4.745\n",
            "After TS- acc:74.829,aft ece:0.17612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JljwD19hADST",
        "outputId": "ca96f235-95ab-40e8-a5c0-29de1aa6987f"
      },
      "source": [
        "seed_everything\n",
        "best_acc = 0\n",
        "best_epoch = 0\n",
        "for epoch in range(10):\n",
        "    train(train_loader, model_ts, criterion, epoch,optimizer)\n",
        "    loss, acc = valid (test_loader,model_ts)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_epoch = epoch\n",
        "        state = {'epoch': epoch, 'transformer': model_ts, 'transformer_optimizer': optimizer}\n",
        "        torch.save(state, '/content/drive/MyDrive/calibration_project/sentiment_analysis/best_base_model_object_ts.pth.tar')\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/calibration_project/sentiment_analysis/best_base_model_object_ts.pth')\n",
        "    print('cur epoch:%d, cur acc:%.5f, best epoch:%d, best acc:%.5f'%(epoch,acc, best_epoch, best_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cur epoch:0, cur acc:75.81967, best epoch:0, best acc:75.81967\n",
            "cur epoch:1, cur acc:75.95628, best epoch:1, best acc:75.95628\n",
            "cur epoch:2, cur acc:76.26366, best epoch:2, best acc:76.26366\n",
            "cur epoch:3, cur acc:75.85383, best epoch:2, best acc:76.26366\n",
            "cur epoch:4, cur acc:73.90710, best epoch:2, best acc:76.26366\n",
            "cur epoch:5, cur acc:75.17077, best epoch:2, best acc:76.26366\n",
            "cur epoch:6, cur acc:74.79508, best epoch:2, best acc:76.26366\n",
            "cur epoch:7, cur acc:75.51230, best epoch:2, best acc:76.26366\n",
            "cur epoch:8, cur acc:75.61475, best epoch:2, best acc:76.26366\n",
            "cur epoch:9, cur acc:75.13661, best epoch:2, best acc:76.26366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHugYMsRpMii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1d385c7-24ad-4cc5-c118-1a6f485faf1d"
      },
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 17 17:00:29 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    28W /  70W |  15106MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0fQACtmGr1-"
      },
      "source": [
        "optial ts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szr353DNqAcY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class SoftTarget(nn.Module):\n",
        "\t'''\n",
        "\tDistilling the Knowledge in a Neural Network\n",
        "\thttps://arxiv.org/pdf/1503.02531.pdf\n",
        "\t'''\n",
        "\tdef __init__(self, T):\n",
        "\t\tsuper(SoftTarget, self).__init__()\n",
        "\t\tself.T = T\n",
        "\n",
        "\tdef forward(self, out_s, out_t):\n",
        "\t\tloss = F.kl_div(F.log_softmax(out_s/self.T, dim=1),\n",
        "\t\t\t\t\t\tF.softmax(out_t/self.T, dim=1),\n",
        "\t\t\t\t\t\treduction='batchmean') * self.T * self.T\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "def train_sd(train_loader, transformer_t, transformer_s, criterion, criterionKD, transformer_optimizer, epoch):    \n",
        "    transformer_s.train()\n",
        "    transformer_t.eval()\n",
        "    sum_loss = 0\n",
        "    count = 0\n",
        "    \n",
        "    for i, pair in enumerate(train_loader): \n",
        "        question = pair.Question.to(device)\n",
        "        reply = pair.Answer.to(device)\n",
        "        reply_input = reply[:, :-1]\n",
        "        reply_target = reply[:, 1:]\n",
        "\n",
        "        # Create mask and add dimensions\n",
        "        question_mask, reply_input_mask = create_masks(question, reply_input)\n",
        "        out_s = transformer_s(question, question_mask, reply_input, reply_input_mask)\n",
        "        with torch.no_grad():\n",
        "            out_t = transformer_t(question, question_mask, reply_input, reply_input_mask)\n",
        "        reply_target = reply_target.reshape(-1)\n",
        "        loss_cls = criterion(out_s.view(-1, ntokens), reply_target)\n",
        "        kd_loss = criterionKD(out_s.view(-1, ntokens), out_t.detach().view(-1, ntokens))\n",
        "        loss = loss_cls + kd_loss\n",
        "        # Backprop\n",
        "        transformer_optimizer.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        transformer_optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}