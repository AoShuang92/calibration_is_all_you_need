{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "LM_V2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AoShuang92/calibration_is_all_you_need/blob/main/LM_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To7mZOiQ0Tyr",
        "outputId": "37a8b471-0501-4813-ebbc-e9b5e42c9854"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "import torch.utils.data\n",
        "import math\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import os\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNBnIaaMfhn8",
        "outputId": "bfe66c23-658d-4889-d216-7ebe4efff6ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayOwOdWQ6fEx"
      },
      "source": [
        "max_length = bptt = 35\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements embeddings of the words and adds their positional encodings. \n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, max_len = max_length):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def create_positinal_encoding(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model).to(device)\n",
        "        for pos in range(max_len):   # for each position of the word\n",
        "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "        pe = pe.unsqueeze(0)   # include the batch size\n",
        "        return pe\n",
        "        \n",
        "    def forward(self, encoded_words):\n",
        "        embedding = self.embed(encoded_words) * math.sqrt(self.d_model)\n",
        "        #print(\"embedding\",embedding.size(),encoded_words.size())\n",
        "        #print(\"pe\",self.pe.size())\n",
        "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
        "        embedding = self.dropout(embedding)\n",
        "        return embedding\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, heads, d_model):\n",
        "        \n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % heads == 0\n",
        "        self.d_k = d_model // heads\n",
        "        self.heads = heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.concat = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        query, key, value of shape: (batch_size, max_len, 512)\n",
        "        mask of shape: (batch_size, 1, 1, max_words)\n",
        "        \"\"\"\n",
        "        # (batch_size, max_len, 512)\n",
        "        query = self.query(query)\n",
        "        key = self.key(key)        \n",
        "        value = self.value(value)   \n",
        "        \n",
        "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
        "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
        "        \n",
        "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
        "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
        "        #scores = torch.matmul(query, key.permute(2,1,0,0)) / math.sqrt(query.size(-1))\n",
        "        #print(\"scores\",scores.size())([35, 8, 2, 2])\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
        "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
        "        weights = self.dropout(weights)\n",
        "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
        "        context = torch.matmul(weights, value)\n",
        "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
        "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
        "        # (batch_size, max_len, h * d_k)\n",
        "        interacted = self.concat(context)\n",
        "        return interacted\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, middle_dim = 2048):\n",
        "        super(FeedForward, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
        "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = self.fc2(self.dropout(out))\n",
        "        return out\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, heads):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, mask):\n",
        "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
        "        interacted = self.layernorm(interacted + embeddings)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        encoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return encoded\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.layernorm = nn.LayerNorm(d_model)\n",
        "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
        "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
        "        query = self.layernorm(query + embeddings)\n",
        "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
        "        interacted = self.layernorm(interacted + query)\n",
        "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
        "        decoded = self.layernorm(feed_forward_out + interacted)\n",
        "        return decoded\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, heads, num_layers, word_map):\n",
        "        super(Transformer, self).__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = word_map\n",
        "        self.embed = Embeddings(self.vocab_size, d_model)#max_len\n",
        "        self.embed_dec = Embeddings(self.vocab_size, d_model,max_len=max_length)\n",
        "        self.encoder = nn.ModuleList([EncoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.decoder = nn.ModuleList([DecoderLayer(d_model, heads) for _ in range(num_layers)])\n",
        "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
        "        \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "    \n",
        "        \n",
        "    def encode(self, src_words, src_mask):\n",
        "        src_embeddings = self.embed(src_words)\n",
        "        for layer in self.encoder:\n",
        "            src_embeddings = layer(src_embeddings, src_mask)\n",
        "        return src_embeddings\n",
        "    \n",
        "    def decode(self, target_words, target_mask, src_embeddings, src_mask):\n",
        "        tgt_embeddings = self.embed(target_words)\n",
        "        for layer in self.decoder:\n",
        "            tgt_embeddings = layer(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
        "        return tgt_embeddings\n",
        "        \n",
        "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
        "        encoded = self.encode(src_words, src_mask)\n",
        "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
        "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
        "        return out\n",
        "\n",
        "class AdamWarmup:\n",
        "    \n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        \n",
        "        self.model_size = model_size\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.optimizer = optimizer\n",
        "        self.current_step = 0\n",
        "        self.lr = 0\n",
        "        \n",
        "    def get_lr(self):\n",
        "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
        "        \n",
        "    def step(self):\n",
        "        # Increment the number of steps each time we call the step function\n",
        "        self.current_step += 1\n",
        "        lr = self.get_lr()\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        # update the learning rate\n",
        "        self.lr = lr\n",
        "        self.optimizer.step()\n",
        "\n",
        "class LossWithLS(nn.Module):\n",
        "\n",
        "    def __init__(self, size, smooth):\n",
        "        super(LossWithLS, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
        "        self.confidence = 1.0 - smooth\n",
        "        self.smooth = smooth\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, prediction, target, mask):\n",
        "        \"\"\"\n",
        "        prediction of shape: (batch_size, max_words, vocab_size)\n",
        "        target and mask of shape: (batch_size, max_words)\n",
        "        \"\"\"\n",
        "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
        "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
        "        mask = mask.float()\n",
        "        mask = mask.reshape(-1)       # (batch_size * max_words)\n",
        "        labels = prediction.data.clone()\n",
        "        labels.fill_(self.smooth / (self.size - 1))\n",
        "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
        "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
        "        return loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Css20WMWyg1x"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/calibration_project/sentiment_analysis/Tweets.csv\"\n",
        "# train_dir = \"/content/combined_qa_train_ID.csv\"\n",
        "# test_dir = \"/content/combined_qa_test_50_ID.csv\"\n",
        "batch_size = 16\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def remove_unnecessary(text):\n",
        "    #remove_URL\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub('', text)\n",
        "\n",
        "    #remove_html\n",
        "    html = re.compile(r'<.*?>')\n",
        "    text = html.sub('', text)\n",
        "\n",
        "    #remove @\n",
        "    text = re.sub('@[^\\s]+','',text)\n",
        "\n",
        "    #remove_emoji\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    #Removes integers \n",
        "    text = ''.join([i for i in text if not i.isdigit()])         \n",
        "    \n",
        "    #remove_punct\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(table)\n",
        "\n",
        "    #Replaces contractions from a string to their equivalents \n",
        "    contraction_patterns = [(r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), \n",
        "                            (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "                            (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "                            (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), \n",
        "                            (r'dont', 'do not'), (r'wont', 'will not')]\n",
        "    \n",
        "    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n",
        "    for (pattern, repl) in patterns:\n",
        "        text, _= re.subn(pattern, repl, text)\n",
        "\n",
        "    #lemmatize_sentence\n",
        "    sentence_words = text.split(' ')\n",
        "    new_sentence_words = list()\n",
        "    \n",
        "    for sentence_word in sentence_words:\n",
        "        sentence_word = sentence_word.replace('#', '')\n",
        "        new_sentence_word = WordNetLemmatizer().lemmatize(sentence_word.lower(), wordnet.VERB)\n",
        "        new_sentence_words.append(new_sentence_word)\n",
        "        \n",
        "    new_sentence = ' '.join(new_sentence_words)\n",
        "    new_sentence = new_sentence.strip()\n",
        "\n",
        "    return new_sentence.lower()\n",
        "\n",
        "\n",
        "def prepare_csv(train_dir, seed=27, val_ratio=0.2):\n",
        "    df_train = pd.read_csv(train_dir,error_bad_lines=False)\n",
        "    idx = np.arange(df_train.shape[0])    \n",
        "    np.random.shuffle(idx)\n",
        "    val_size = int(len(idx) * val_ratio)\n",
        "    if not os.path.exists('cache'): # cache is tem memory file \n",
        "        os.makedirs('cache')\n",
        "    \n",
        "    df_train.iloc[idx[val_size:], :][['tweet_id', 'text']].to_csv(\n",
        "        'cache/dataset_train.csv', index=False)\n",
        "    \n",
        "    df_train.iloc[idx[:val_size], :][['tweet_id',  'text']].to_csv(\n",
        "        'cache/dataset_val.csv', index=False)  \n",
        "    \n",
        "    \n",
        "#prepare_csv(train_dir) \n",
        "\n",
        "def get_iterator(dataset, batch_size, train=True,\n",
        "                 shuffle=True, repeat=False, device=None): \n",
        "    dataset_iter = data.Iterator(\n",
        "        dataset, batch_size=batch_size, device=device,\n",
        "        train=train, shuffle=shuffle, repeat=repeat,\n",
        "        sort=False)  \n",
        "    return dataset_iter\n",
        "\n",
        "def batchify(TEXT,data, bsz):\n",
        "    #print(type(data))\n",
        "    #print(\"data\",data.examples[0].text)\n",
        "    data = TEXT.numericalize([data])\n",
        "    print(\"data0\",data.size())\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    #nbatch = len(data) // bsz\n",
        "    print(\"nbatch\",nbatch)\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    #print(nbatch)\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    print(\"data1\",data.size())\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    print(\"data2\",data.size())\n",
        "    #print(data.size())\n",
        "    return data.to(device)\n",
        "\n",
        "def get_dataset(fix_length=max_length, lower=False, vectors=None,train_dir = train_dir, batch_size=batch_size, device=None): \n",
        "    df_train = pd.read_csv(train_dir,error_bad_lines=False)\n",
        "    df_train['text'] = df_train['text'].apply(lambda x: remove_unnecessary(x))\n",
        "    \n",
        "    prepare_csv(train_dir)\n",
        "    if vectors is not None:\n",
        "        lower=True\n",
        "\n",
        "    TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"spacy\"),init_token='<sos>',eos_token='<eos>',lower=True,\n",
        "                      fix_length=fix_length)\n",
        "    \n",
        "    LABEL = data.Field(use_vocab=False, sequential=False, dtype=torch.float16)\n",
        "    ID = data.Field(use_vocab=False, sequential=False, dtype=torch.float16) \n",
        "    \n",
        "\n",
        "\n",
        "    train_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_train.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"tweet_id\",ID),('text', TEXT), ('airline_sentiment', LABEL)]) \n",
        "    test_temps = data.TabularDataset(\n",
        "        path='/content/cache/dataset_val.csv', format='csv', skip_header=True,\n",
        "        fields=[(\"tweet_id\",ID),('text', TEXT) , ('airline_sentiment', LABEL)]) \n",
        "\n",
        "    TEXT.build_vocab(train_temps,test_temps, max_size=20000,\n",
        "        min_freq=10, vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(train_temps, test_temps)\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    all_words = TEXT.vocab.itos\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    print(\"ntokens:\",ntokens,vocab_size,len(all_words))\n",
        "\n",
        "    TEXT.build_vocab(train_temps,max_size=20000,\n",
        "        min_freq=10, vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(train_temps)\n",
        "    word_embeddings_train = TEXT.vocab.vectors\n",
        "    words_train = TEXT.vocab.itos\n",
        "    vocab_size_train = len(TEXT.vocab)\n",
        "    ntokens_train = len(TEXT.vocab.stoi)\n",
        "    print(\"ntokens_train:\",ntokens_train,vocab_size_train,len(words_train))\n",
        "\n",
        "    TEXT.build_vocab(test_temps,vectors=GloVe(name='6B', dim=300))\n",
        "    ID.build_vocab(test_temps)\n",
        "    word_embeddings_test = TEXT.vocab.vectors\n",
        "    words_test = TEXT.vocab.itos\n",
        "    vocab_size_test = len(TEXT.vocab)\n",
        "    ntokens_test = len(TEXT.vocab.stoi)\n",
        "    print(\"ntokens_train:\",ntokens_test,vocab_size_test,len(words_test))\n",
        "\n",
        "    train_loader = batchify(TEXT, words_train, batch_size)\n",
        "    test_loader = batchify(TEXT,words_test, batch_size)\n",
        "\n",
        "    print('Train samples:%d'%(len(train_temps)), 'Valid samples:%d'%(len(test_temps)),'Train minibatch nb:%d'%(len(train_loader)),\n",
        "            'Valid minibatch nb:%d'%(len(test_loader)))\n",
        "    return vocab_size, word_embeddings, ntokens, train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRSLxdHJy9QA",
        "outputId": "3131803e-39b8-45d6-aaa4-5f0fef768cde"
      },
      "source": [
        "vocab_size, word_embeddings, ntokens, train_loader, test_loader= get_dataset(fix_length=max_length,train_dir = train_dir, batch_size=batch_size)\n",
        "print(ntokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ntokens: 1985 1985 1985\n",
            "ntokens_train: 1722 1722 1722\n",
            "ntokens_train: 6360 6360 6360\n",
            "data0 torch.Size([1722, 1])\n",
            "nbatch 107\n",
            "data1 torch.Size([1712, 1])\n",
            "data2 torch.Size([107, 16])\n",
            "data0 torch.Size([6360, 1])\n",
            "nbatch 397\n",
            "data1 torch.Size([6352, 1])\n",
            "data2 torch.Size([397, 16])\n",
            "Train samples:11712 Valid samples:2928 Train minibatch nb:107 Valid minibatch nb:397\n",
            "1985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XntztE9I13eF"
      },
      "source": [
        "def create_masks(question, reply_input):\n",
        "    \n",
        "    def subsequent_mask(size):\n",
        "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
        "        return mask.unsqueeze(0)\n",
        "    \n",
        "    question_mask = question!=0\n",
        "    question_mask = question_mask.to(device)\n",
        "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
        "     \n",
        "    reply_input_mask = reply_input!=0\n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
        "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
        "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
        "    \n",
        "    return question_mask, reply_input_mask\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(max_length, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa6Gss1K19Vo",
        "outputId": "bac1a89a-bbb6-443f-b0b6-7719a7ebc444"
      },
      "source": [
        "lr = 5.0\n",
        "d_model = 512\n",
        "heads = 8\n",
        "num_layers = 3\n",
        "\n",
        "\n",
        "model = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = ntokens)\n",
        "model = model.to(device)\n",
        "adam_optimizer = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_loader.size(0) - 1, bptt)):\n",
        "        \n",
        "        data, targets = get_batch(train_loader, i)\n",
        "        data = data.permute(1,0)\n",
        "        targets = targets.permute(1,0)\n",
        "        src_mask,tar_mask = create_masks(data,targets)\n",
        "        output = model(data, src_mask, targets, tar_mask)\n",
        "        targets = targets.reshape(-1)\n",
        "        loss = criterion(output.view(-1,ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 50\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            #elapsed = time.time() - start_time\n",
        "            #print(\"epoch:\",epoch, \"loss:\",cur_loss)\n",
        "            # print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "            #       'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "            #       'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "            #         epoch, i, len(train_loader) // max_length, scheduler.get_lr()[0],\n",
        "            #         elapsed * 1000 / log_interval,\n",
        "            #         cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for batch, i in enumerate(range(0, data_source.size(0) - 1, bptt)):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            data = data.permute(1,0)\n",
        "            targets = targets.permute(1,0)\n",
        "            src_mask,tar_mask = create_masks(data,targets)\n",
        "            output = eval_model(data, src_mask,targets,tar_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            targets = targets.reshape(-1)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "epochs = 50 # The number of epochs\n",
        "best_model = None\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "best_epoch = 0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, test_loader)\n",
        "    #print('-' * 89)\n",
        "    # print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "    #       'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "    #                                  val_loss, math.exp(val_loss)))\n",
        "    #print(\"end of epoch:\",epoch, \"valid_loss:\", val_loss, \"valid_ppl:\",math.exp(val_loss))\n",
        "    #print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "        best_epoch = epoch\n",
        "        \n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/calibration_project/sentiment_analysis/best_language_model.pth')\n",
        "    print(\"epoch:\",epoch,\"loss:\",val_loss, \"best_epoch\",best_epoch,\"best_loss:\",best_val_loss)\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 6.114708121496302 best_epoch 1 best_loss: 6.114708121496302\n",
            "epoch: 2 loss: 10.22878407097348 best_epoch 1 best_loss: 6.114708121496302\n",
            "epoch: 3 loss: 5.695517930669032 best_epoch 3 best_loss: 5.695517930669032\n",
            "epoch: 4 loss: 4.741299306466683 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 5 loss: 4.7504697610403746 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 6 loss: 5.161473902733878 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 7 loss: 5.2604770029470815 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 8 loss: 6.218741768795722 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 9 loss: 7.828553333233938 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 10 loss: 9.137231724862835 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 11 loss: 4.977654687624245 best_epoch 4 best_loss: 4.741299306466683\n",
            "epoch: 12 loss: 4.670363020957578 best_epoch 12 best_loss: 4.670363020957578\n",
            "epoch: 13 loss: 6.5080383922004215 best_epoch 12 best_loss: 4.670363020957578\n",
            "epoch: 14 loss: 4.63007593518905 best_epoch 14 best_loss: 4.63007593518905\n",
            "epoch: 15 loss: 11.929944763960123 best_epoch 14 best_loss: 4.63007593518905\n",
            "epoch: 16 loss: 4.742143043731612 best_epoch 14 best_loss: 4.63007593518905\n",
            "epoch: 17 loss: 4.607629363470102 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 18 loss: 4.760225990043038 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 19 loss: 4.83328678043744 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 20 loss: 5.397680345685731 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 21 loss: 5.128703148917084 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 22 loss: 4.852995700205252 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 23 loss: 4.8561006114076415 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 24 loss: 5.137755493474674 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 25 loss: 4.673068330488132 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 26 loss: 5.056193111507037 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 27 loss: 5.243090525231592 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 28 loss: 4.972615870507315 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 29 loss: 5.191960109099177 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 30 loss: 5.029830427873529 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 31 loss: 4.668904350610786 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 32 loss: 4.651814739819398 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 33 loss: 4.740089474743559 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 34 loss: 4.716770705982626 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 35 loss: 4.713945034502723 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 36 loss: 4.706222573006123 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 37 loss: 4.683773283436705 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 38 loss: 4.700886423047869 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 39 loss: 4.73063919805085 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 40 loss: 4.6834999065059435 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 41 loss: 4.78220602331574 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 42 loss: 4.729909261067708 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 43 loss: 4.791669617475748 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 44 loss: 4.784293643087528 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 45 loss: 4.833544743273397 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 46 loss: 4.815512193674956 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 47 loss: 4.6902701496774615 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 48 loss: 4.844628409271628 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 49 loss: 4.782216778238311 best_epoch 17 best_loss: 4.607629363470102\n",
            "epoch: 50 loss: 4.818788445936208 best_epoch 17 best_loss: 4.607629363470102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffMx_sEocJn3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}